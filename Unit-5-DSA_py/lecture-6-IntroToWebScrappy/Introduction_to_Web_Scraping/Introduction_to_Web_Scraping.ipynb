{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a name=\"sections\"></a></p>\n",
    "<br>\n",
    "<br>\n",
    "# Sections\n",
    "\n",
    "- <a href=\"#intro\">Introduction to Beautiful Soup</a><br>\n",
    "    - <a href=\"#web\">What is web scraping</a><br>\n",
    "    - <a href=\"#html\">Introduction to HTML</a><br>\n",
    "    - <a href=\"#beautiful\">Basics of Beautiful Soup</a><br>\n",
    "\n",
    "- <a href=\"#example\">Examples</a><br>\n",
    "    - <a href=\"#calendar\">Python User Group Calendar</a><br>\n",
    "    - <a href=\"#yelp\">Scrape Yelp Reviews</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a name=\"web\"></a></p>\n",
    "## What is web scraping?\n",
    "\n",
    "- HTML is short for **HyperText Markup Language**. It's a language for presenting content on the Web.\n",
    "\n",
    "- Plain text is turned into an HTML document by **tags** that are then interpreted by a browser.\n",
    "\n",
    "- Using BeautifulSoup, you can easily extract the tag values from HTML source code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beautiful Soup VS Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the source code of hi.html\n",
    "!cat data/hi.html\n",
    "# Windows user\n",
    "# !type data\\hi.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "- Extract the characters between the title tags. \n",
    "\n",
    "\n",
    "- In this case it's `Hi` (`<title>Hi</title>`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Solution using Regular Expressions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "hi_path = 'data/hi.html'\n",
    "with open(hi_path, 'r') as f:\n",
    "    hi = f.read()\n",
    "    print(re.findall('<title>(.*)</title>', hi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Solution using BeautifulSoup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "with open(hi_path, 'r') as f:\n",
    "    hi = f.read()\n",
    "    hi = BeautifulSoup(hi, 'html.parser')\n",
    "    print(hi.title) # find the title tag\n",
    "    print(hi.title.string)  # find the value of tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compared with regular expressions:**\n",
    "    \n",
    "- Beautiful Soup's syntax is much simpler, while regular expressions are more flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a name=\"html\"></a></p>\n",
    "## Introduction to HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag\n",
    "\n",
    "- The `<title>` tags in this example designate the enclosed text as the title to be displayed in the head of the browser tab.\n",
    "![hi](pic/hi.png)\n",
    "\n",
    "- Tags are always enclosed by `<` and `>` to distinguish them from the content. \n",
    "- A pair of tags consist of start and end tags which carry the same name, but the end tag is preceded by a slash `/` ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Values\n",
    "\n",
    "Values are the content between start tags and end tags.\n",
    "\n",
    "- **Example**\n",
    "\n",
    "`<title>Hi</title>`: It's a title tag with a value of `Hi`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes\n",
    "Tags have another feauture called attributes.\n",
    "\n",
    "- **Example**\n",
    "\n",
    "`<a href='http://www.crummy.com/software/BeautifulSoup/'>Hello, beautifulsoup!</a>`\n",
    "\n",
    "The anchor tag `<a>` with an attribute `href` and hyperlinkâ€”http://www.crummy.com/software/BeautifulSoup/. It creates an association of text points to another address (a hyperlink)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree structure\n",
    "- The first tag in the example is the `<html>` tag. \n",
    "\n",
    "- Between the `<html>` tags, several tags are opened and closed again: `<head>, <title>` , and\n",
    "`<body>, <a>`.\n",
    "\n",
    "    - The `<head>` and `<body>` tags are directly enclosed by the `<html>` tag. \n",
    "    - The `<title>` tag is enclosed by the `<head>` tag.\n",
    "    - The `<a>` tag is enclosed by the `<body>` tag.\n",
    "\n",
    "\n",
    "- A good way to describe the multiple layers of an HTML document is the tree analogy. \n",
    "![html](pic/html.png)\n",
    "\n",
    "- The `html` tag is the root tag that splits into two branches, `<head>` and `<body>`; `<head>` is followed by another branch called `<title>`; `<body>` is followed by another branch called `<a>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a name=\"beautiful\"></a></p>\n",
    "## Basics of Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `prettify()` method adds indentations so that it will help you understand the tree structure of the html document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# open a local file and parse the plain text by BeautifulSoup directly\n",
    "with open(hi_path, 'r') as f:\n",
    "    hi = f.read()\n",
    "    hi = BeautifulSoup(hi, 'html.parser')\n",
    "    print(type(hi)) # get a bs4.BeautifulSoup object\n",
    "    print('\\n')\n",
    "    print(hi.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Names, Values, and Attributes\n",
    "\n",
    "Beautiful Soup can extract the `name`, `value` and `attributes` of tags. The corresponding methods are:\n",
    "- name\n",
    "- string\n",
    "- attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"The name of a tags is: \", hi.a.name)\n",
    "print(\"The value of a tags is: \", hi.a.string)\n",
    "print(\"The attribute of a tags is: \", hi.a.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_text() & get()\n",
    "- For tags that have child tags the string does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(hi.html.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the get_text method instead. The `get_text()` method will extract all the contents of child tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(hi.html.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `get()` is used to find the attribute of a tag. For example, we can get the href of tag a using the following code. \n",
    "\n",
    "- It is the same as run `hi.a.attrs` first and then find the value of key `href` from the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(hi.a.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(hi.a.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find() & find_all()\n",
    "The functions `find` and `findall` are flexible for finding tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat data/article.html\n",
    "# Windows user\n",
    "# !type data\\article.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![article](pic/article.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article_path = 'data/article.html'\n",
    "with open(article_path, 'r') as f:\n",
    "    article = f.read()\n",
    "    article = BeautifulSoup(article, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Return only the first `p` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(article.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `find()` returns the first p tags, which is equivalent to article.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(article.find('p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `find_all()` returns all p tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(article.find_all('p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To find the tags that have specific attributes, you can pass a dictionary as the `attrs` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(article.find_all('h1', attrs={'id':'one'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can also specify a function to extract a list of Tag objects that match the given criteria.\n",
    "- It is the same as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the tags whose attribute id equals 'one'\n",
    "print(article.find_all(lambda tag: tag.get('id') == 'one'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a name=\"example\"></a></p>\n",
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a name=\"calendar\"></a></p>\n",
    "### Python User Group Calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the time, location, and event titles from this web page [Python User Group Calendar](https://www.python.org/events/python-user-group/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=pic/events.png width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the examples we discussed before, we saved the html document locally. However, you don't want to download all the pages and then start scraping for your web scraping project.\n",
    "- The [Requests package](http://docs.python-requests.org/en/master/) we are using here is well designed and very popular in the industry. It makes http requests easy to use with Python.\n",
    "- The `get` method we are using here is one type of [http request](https://www.tutorialspoint.com/http/http_methods.htm). It is most often used to retrieve information from the web server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.get('https://www.python.org/events/python-user-group/')\n",
    "text = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(text.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title\n",
    "Titles are in `h3` tags with an attribute `class=\"event-title\"`.\n",
    "<img src=pic/title.png width=900/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titleTags = text.find_all('h3', {'class': \"event-title\"})\n",
    "titleTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titleString = [tag.string for tag in titleTags]\n",
    "titleString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time\n",
    "Times are in the `time` tags that have `datetime` attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![time](pic/time.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timeTags = text.find_all(lambda tag: 'datetime' in tag.attrs)\n",
    "timeTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timeString = [tag.get('datetime') for tag in timeTags]\n",
    "timeString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Location\n",
    "Locations are in `span` tags with the attribute `class=\"event-location\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=pic/location.png width=900/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "locationTags = text.find_all(\"span\", {\"class\": \"event-location\"})\n",
    "locationTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "locationString = [tag.string for tag in locationTags]\n",
    "locationString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping Project Workflow\n",
    "- We have been lucky so far because there is no missing values on this page. But what if the location of one event is missing? There is no way for us to locate it from three lists of different length.\n",
    "- The general workflow of a web scraping project is like the following:\n",
    " - Find the unique attribute that will locate the **top level** tags that you are interested in.\n",
    "     - Each tag could be a listing, review, item...\n",
    "     - **one unique tag -> one row in csv file**\n",
    " - We want to locate the event tag that its child tags contain the title, datetime and location that you want to save as columns in a csv file.\n",
    " - Then you go levels deeper to find the child tags of each event. If there is something missing there, you just replace it with an empty string.\n",
    " - The event tags have a unique  attribute **class=list-recent-events menu**.\n",
    " - Next question is: what is the best data structure to represent one single event?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save all the event is a list\n",
    "result = []\n",
    "# Save all the ul tags, each ul is a section of the page\n",
    "uls = text.find_all('ul', {'class': 'list-recent-events menu'})\n",
    "for ul in uls:\n",
    "    # Save all the li tags, each li is an event\n",
    "    lis = ul.find_all('li')\n",
    "    for li in lis:\n",
    "        # Initialize an empty dictionary for each event\n",
    "        event = {}\n",
    "        # Using try/except to avoid errors caused by missing values\n",
    "        try:\n",
    "            title = li.find('a').string\n",
    "        except:\n",
    "            continue       \n",
    "        try:\n",
    "            time = li.find('time').get('datetime')\n",
    "        except:\n",
    "            time = \"\"\n",
    "        try:\n",
    "            location = li.find('span', {'class':'event-location'}).string.strip()\n",
    "        except:\n",
    "            location = \"\"\n",
    "        \n",
    "        # Assign the values in the dictionary\n",
    "        event['location'] = location\n",
    "        event['time'] = time\n",
    "        event['title'] = title\n",
    "        result.append(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a name=\"yelp\"></a></p>\n",
    "## Scrape Yelp Reviews\n",
    "- Let's apply what we have learned to a more complicated example - scrape Yelp reviews. \n",
    "- Our task is to scrape all the reviews of the ABC Kitchen Restaurant on Yelp. https://www.yelp.com/biz/abc-kitchen-new-york\n",
    "- You can easily extend this code to all the restaurants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Find the pattern of url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we added `User-Agent` to the header of our request. It is because sometimes the web server will check the different fields of the header to block robot scrapers. \n",
    "- `User-Agent` is the most common one because it is specific to your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
    "            }\n",
    "\n",
    "response = requests.get('https://www.yelp.com/biz/abc-kitchen-new-york', headers=headers)\n",
    "text = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you go to the second page, you can see the url becomes https://www.yelp.com/biz/abc-kitchen-new-york?start=20\n",
    "- Similarly, the url to the thid page: https://www.yelp.com/biz/abc-kitchen-new-york?start=40\n",
    "- But how do we find out the url of the last page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "num_reviews = text.find('span', attrs={'class': 'review-count rating-qualifier'}).string\n",
    "num_reviews = int(re.findall('\\d+', num_reviews)[0])\n",
    "print(num_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = []\n",
    "for i in range(0, num_reviews, 20):\n",
    "    url_list.append('https://www.yelp.com/biz/abc-kitchen-new-york?start='+str(i))\n",
    "print(url_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Find all the review divs on the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = text.find_all('div', attrs={'class':'review review--with-sidebar'})\n",
    "print(len(reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Scrape the detail information\n",
    "\n",
    "For debugging purpose, we usually test it out on one review and then apply to the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = reviews[0]\n",
    "\n",
    "# Username\n",
    "username = review.find('a', attrs={'class': 'user-display-name js-analytics-click'}).string\n",
    "print(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location\n",
    "location = review.find('li', attrs={'class': 'user-location responsive-hidden-small'}).get_text()\n",
    "print(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating\n",
    "rating = review.find('img', attrs={'class': 'offscreen'}).get('alt')\n",
    "rating = float(re.findall('\\d+', rating)[0])\n",
    "print(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Date\n",
    "date = review.find('span', attrs={'class': 'rating-qualifier'}).get_text()\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Content\n",
    "content = review.find('p').get_text()\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Apply to all the reviews and save them to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "# Windows using text encoding when opening the file by default.\n",
    "# Override it to 'utf-8' will save lots of encoding issues.\n",
    "with open('reviews.csv', 'w', encoding='utf-8') as csvfile:\n",
    "    review_writer = csv.writer(csvfile)\n",
    "    for review in reviews:\n",
    "        dic = {}\n",
    "        username = review.find('a', attrs={'class': 'user-display-name js-analytics-click'}).string\n",
    "        location = review.find('li', attrs={'class': 'user-location responsive-hidden-small'}).get_text().strip()\n",
    "        date = review.find('span', attrs={'class': 'rating-qualifier'}).get_text().strip()\n",
    "        rating = review.find('img', attrs={'class': 'offscreen'}).get('alt')\n",
    "        rating = float(re.findall('\\d+', rating)[0])\n",
    "        content = review.find('p').text\n",
    "        dic['username'] = username\n",
    "        dic['location'] = location\n",
    "        dic['date'] = date\n",
    "        dic['rating'] = rating\n",
    "        dic['content'] = content\n",
    "        review_writer.writerow(dic.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Apply to all the pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "def scrape_single_page(reviews, csvwriter):\n",
    "    for review in reviews:\n",
    "        dic = {}\n",
    "        username = review.find('a', attrs={'class': 'user-display-name js-analytics-click'}).text\n",
    "        location = review.find('li', attrs={'class': 'user-location responsive-hidden-small'}).text.strip()\n",
    "        date = review.find('span', attrs={'class': 'rating-qualifier'}).text.strip()\n",
    "        rating = review.find('img', attrs={'class': 'offscreen'}).get('alt')\n",
    "        rating = rating = float(re.findall('\\d+', rating)[0])\n",
    "        content = review.find('p').text\n",
    "        dic['username'] = username\n",
    "        dic['location'] = location\n",
    "        dic['date'] = date\n",
    "        dic['rating'] = rating\n",
    "        dic['content'] = content\n",
    "        csvwriter.writerow(dic.values())\n",
    "    \n",
    "\n",
    "with open('reviews.csv', 'w', encoding='utf-8') as csvfile:\n",
    "    review_writer = csv.writer(csvfile)\n",
    "    for index, url in enumerate(url_list):\n",
    "        response = requests.get(url, headers=headers)\n",
    "        text = BeautifulSoup(response.text, 'html.parser')\n",
    "        reviews = text.find_all('div', attrs={'class':'review review--with-sidebar'})\n",
    "        scrape_single_page(reviews, review_writer)\n",
    "        # Random sleep to avoid getting banned from the server\n",
    "        time.sleep(random.randint(1,3))\n",
    "        # Log the progress\n",
    "        print('Finished page ' + str(index + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
